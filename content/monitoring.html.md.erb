---
title: Monitoring VMware GemFire for TAS Service Instances
---

 <!--
 Copyright (c) VMware, Inc. 2022. All rights reserved.
 Licensed to the Apache Software Foundation (ASF) under one or more contributor license
 agreements. See the NOTICE file distributed with this work for additional information regarding
 copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance with the License. You may obtain a
 copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software distributed under the License
 is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 or implied. See the License for the specific language governing permissions and limitations under
 the License.
 -->

This topic describes options to use when running VMware GemFire for TAS.

VMware GemFire for Tanzu Application Service clusters and brokers emit service metrics.
You can use any tool that has a corresponding Cloud Foundry nozzle to read and monitor these metrics in real time,
including [VMware Aria Operations for Applications](https://docs.wavefront.com/gemfire.html#vmware-gemfire-for-tanzu-application-service-setup).

As an app developer, when you opt to use a data service, you should be prepared to:

- monitor the state of that service
- triage issues that occur with that service
- be notified of any concerns

If you believe an issue relates to the underlying infrastructure (network, CPU, memory, or disk),
you will need to capture evidence and notify your platform team. The metrics described in this
section can help in characterizing the performance and resource consumption of your service
instance.

GemFire for TAS does not publish locator metrics for service
instances created as Co-located Single VM plans or Co-located Multi VM plans.

## <a id="service-instance-metrics"></a> Service Instance Metrics

In the descriptions of the metrics, KPI stands for Key Performance Indicator.

### <a id="service-instance-MemberCount"></a> Member Count

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>serviceinstance_MemberCount</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Returns the number of members in the distributed system.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>less than the manifest member count</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>This depends on the expected member count, which is available in the BOSH manifest. If the number expected is different from the number emitted, this is a critical situation that may lead to data loss, and the reasons for node failure should be investigated by examining the service logs.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>Member loss due to any reason can potentially cause data loss.</td>
   </tr>
</table>

### <a id="service-instance-TotalHeapSize"></a> Total Available Heap Size

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>serviceinstance_TotalHeapSize</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Returns the total available heap, in megabytes, across all instance members.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>pulse</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.</td>
   </tr>
</table>

### <a id="service-instance-UsedHeapSize"></a> Total Used Heap Size

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>serviceinstance_UsedHeapSize</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Returns the total heap used across all instance members, in megabytes.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>pulse</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.</td>
   </tr>
</table>

### <a id="service-instance-UnusedHeapSizePercentage"></a> Total Available Heap Size as a Percentage

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>serviceinstance_UnusedHeapSizePercentage</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Returns the proportion of total available heap across all instance members, expressed as a percentage.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>percent</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>compound metric</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>40%</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>10%</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>If this is a spike due to eviction catching up with insert frequency, then customers need to keep a close watch that it should not hit the RED marker. If there is no eviction, then horizontal scaling is suggested.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.</td>
   </tr>
</table>

## <a id="per-member-metrics"></a> Per Member Metrics

### <a id="member-UsedMemoryPercentage"></a> Memory Used as a Percentage

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_UsedMemoryPercentage</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>RAM being consumed.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>percent</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>average</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>75%</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>85%</td>
   </tr>
</table>

### <a id="member-GC-count"></a> Count of Java Garbage Collections

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_GarbageCollectionCount</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The number of times that garbage has been collected over a rolling window of time. The rolling window is updated and the count is recalculated every second.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Sum over last 10 minutes</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>Check the number of queries run against the system, which increases the deserialization of objects and increases garbage.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the frequency of garbage collection is high, the system might see high CPU usage, which causes delays in the cluster.</td>
   </tr>
</table>

### <a id="member-cpuusage"></a> CPU Utilization Percentage

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_HostCpuUsage</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>This member's process CPU utilization, expressed as a percentage.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>percent</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>average</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>85%</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>95%</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>If this is not happening with high GC activity, the system is reaching its limits. Horizontal scaling might help.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>High CPU usage causes delayed responses and can also make the member non-responsive. This can cause the member to be kicked out of the cluster, potentially leading to data loss.</td>
   </tr>
</table>

### <a id="member-get-avg-latency"></a> Average Latency of Get Operations

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_GetsAvgLatency</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The average latency of cache get operations, in nanoseconds.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>average</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>If this is not happening with high GC activity, the system is reaching its limit. Horizontal scaling might help.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>It is a good indicator of the overall responsiveness of the system. If this number is high, the service administrator should diagnose the root cause.</td>
   </tr>
</table>

### <a id="member-put-avg-latency"></a> Average Latency of Put Operations

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_PutsAvgLatency</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The average latency of cache put operations, in nanoseconds.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>average</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>If this is not happening with high GC activity, the system is reaching its limit. Horizontal scaling might help.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>It is a good indicator of the overall responsiveness of the system. If this number is high, the service administrator should diagnose the root cause.</td>
   </tr>
</table>

### <a id="member-jvm-pauses"></a> JVM pauses

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_JVMPauses</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The quantity of JVM pauses.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Sum over 2 seconds</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>Dependent on the IaaS and app use case.</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>Check the cached object size; if it is greater than 1 MB, you may be hitting the limitation on JVM to garbage collect this object. Otherwise, you may be hitting the utilization limit on the cluster, and will need to scale up to add more memory to the cluster.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>Due to a JVM pause, the member stops responding to "are-you-alive" messages, which may cause this member to be kicked out of the cluster.</td>
   </tr>
</table>

### <a id="member-file-descriptor-limit"></a> File Descriptor Limit

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_FileDescriptorLimit</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The maximum number of open file descriptors allowed for the member's host operating system.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>pulse</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.</td>
   </tr>
</table>

### <a id="member-total-file-descriptor-open"></a> Open File Descriptors

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_TotalFileDescriptorOpen</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The current number of open file descriptors.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>pulse</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.</td>
   </tr>
</table>

### <a id="member-file-descriptor-remaining"></a> Quantity of Remaining File Descriptors

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_FileDescriptorRemaining</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The number of available file descriptors.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Every second</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>compound metric</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>1000</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>100</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>Scale horizontally to increase capacity.</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>If the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.</td>
   </tr>
</table>

### <a id="member-reply-waits-in-progress"></a> Threads Waiting for a Reply

<table>
   <tr><th colspan="2" style="text-align: center;"><br> <code>member_ReplyWaitsInProgress</code><br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The quantity of threads currently waiting for a reply.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Suggested measurement</th>
      <td>Average over the past 10 seconds</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>pulse</td>
   </tr>
   <tr>
      <th>Warning Threshold</th>
      <td>1</td>
   </tr>
   <tr>
      <th>Critical Threshold</th>
      <td>10</td>
   </tr>
   <tr>
      <th>Suggested Actions</th>
      <td>If the value does not average to zero over the sample interval, then the member is waiting for responses from other members. There are two possible explanations: either another member is unhealthy, or the network is dropping packets. Check other members' health, and check for network issues.
</td>
   </tr>
   <tr>
      <th>Why a KPI?</th>
      <td>Unhealthy members are excluded from the cluster, possibly leading to data loss.</td>
   </tr>
</table>


## <a id="gateway-sender-metrics"></a> Gateway Sender and Gateway Receiver Metrics

These are metrics emitted through the CF Nozzle for gateway senders and gateway receivers.

#### <a id="gs-event-queue-size"></a> Queue Size for the Gateway Sender

<table>
   <tr><th colspan="2" style="text-align: center;"><code>gatewaySender_&lt;sender-id&gt;_EventQueueSize</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The current size of the gateway sender queue. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

### <a id="gs-event-received-rate"></a> Events Received at the Gateway Sender

<table>
   <tr><th colspan="2" style="text-align: center;"><code>gatewaySender_&lt;sender-id&gt;_EventsReceivedRate</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>A count of the events coming from the region to which the gateway sender is attached. It is the count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway sender was created. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

### <a id="gs-events-queued-rate"></a> Events Queued by the Gateway Sender

<table>
   <tr><th colspan="2" style="text-align: center;"><code>gatewaySender_&lt;sender-id&gt;_EventsQueuedRate</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>A count of the events queued on the gateway sender from the region. This quantity of events might be lower than the quantity of events received, as not all received events are queued. It is a count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway sender was created.</td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

### <a id="gr-event-received-rate"></a> Events Received by the Gateway Receiver

<table>
   <tr><th colspan="2" style="text-align: center;"><code>gatewayReceiver_EventsReceivedRate</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>A count of the events received from the gateway sender which will be applied to the region on the gateway receiver's site. It is the count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway receiver was created. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

## <a id="disk-metrics"></a> Disk Metrics

These are metrics emitted through the CF Nozzle for disks.

### <a id="disk-writes-avg-latency"></a> Average Latency of Disk Writes

<table>
   <tr><th colspan="2" style="text-align: center;"><code>diskstore_DiskWritesAvgLatency</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The average latency of disk writes in nanoseconds. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>time in nanoseconds</td>
   </tr>
</table>

### <a id="total-disk-space"></a> Quantity of Bytes on Disk

<table>
   <tr><th colspan="2" style="text-align: center;"><code>diskstore_TotalSpace</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The total number of bytes on the attached disk. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

### <a id="total-available-disk-space"></a> Quantity of Available Bytes on Disk

<table>
   <tr><th colspan="2" style="text-align: center;"><code>diskstore_UseableSpace</code> </th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The total number of bytes of available space on the attached disk. </td>
   </tr>
   <tr>
      <th>Metric Type</th>
      <td>number</td>
   </tr>
   <tr>
      <th>Measurement Type</th>
      <td>count</td>
   </tr>
</table>

## <a id="experimental-metrics"></a> Experimental Metrics

These metrics are experimental.
Any of these metrics may be removed without advance notice,
and the current name of any of these metrics may change.
Expect changes to these metrics.

These experimental metrics are specific to a region (`REGION-NAME`):

- `region.REGION-NAME.BucketCount`
- `region.REGION-NAME.CreatesRate`
- `region.REGION-NAME.DestroyRate`
- `region.REGION-NAME.EntrySize`
- `region.REGION-NAME.FullPath`
- `region.REGION-NAME.GetsRate`
- `region.REGION-NAME.NumRunningFunctions`
- `region.REGION-NAME.PrimaryBucketCount`
- `region.REGION-NAME.PutAllRate`
- `region.REGION-NAME.PutLocalRate`
- `region.REGION-NAME.PutRemoteRate`
- `region.REGION-NAME.PutsRate`
- `region.REGION-NAME.RegionType`
- `region.REGION-NAME.SystemRegionEntryCount`
- `region.REGION-NAME.TotalBucketSize`
- `region.REGION-NAME.TotalRegionCount`
- `region.REGION-NAME.TotalRegionEntryCount`

These experimental metrics are specific to a member:

- `member.AverageReads`
- `member.AverageWrites`
- `member.BytesReceivedRate`
- `member.BytesSentRate`
- `member.CacheServer`
- `member.ClientConnectionCount`
- `member.CreatesRate`
- `member.CurrentHeapSize`
- `member.DeserializationRate`
- `member.DestroysRate`
- `member.FunctionExecutionRate`
- `member.GetRequestRate`
- `member.GetsRate`
- `member.MaxMemory`
- `member.MemberUpTime`
- `member.NumThreads`
- `member.PDXDeserializationRate`
- `member.PutAllRate`
- `member.PutRequestRate`
- `member.PutsRate`
- `member.TotalBucketCount`
- `member.TotalHitCount`
- `member.TotalMissCount`
- `member.TotalPrimaryBucketCount`

## <a id="bosh-errand-report"></a> Total Memory Consumption

The BOSH `mem-check` errand calculates and outputs
the quantity of memory used across all PCC service instances.
This errand helps VMware GemFire for Tanzu Application Service (GemFire for TAS) operators monitor resource costs,
which are based on memory usage.

From the director, run a BOSH command of the form:

``` pre
bosh -d <service broker name> run-errand mem-check
```

With this command:

``` pre
bosh -d cloudcache-service-broker run-errand mem-check
```

Here is an anonymized portion of example output from the `mem-check` errand
for a two cluster deployment:

``` pre
           Analyzing deployment xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx1...
           JVM heap usage for service instance xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx1
           Used Total = 1204 MB
           Max Total = 3201 MB

           Analyzing deployment xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2...
           JVM heap usage for service instance xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2
           Used Total = 986 MB
           Max Total = 3201 MB

           JVM heap usage for all clusters everywhere:
           Used Global Total = 2390 MB
           Max Global Total = 6402 MB
```
